import tensorflow as tf
import numpy as np
#############################################################################################################
# Convolution Layer methods

def conv2d_p(name, x, w=None, num_filters=16, kernel_size=(3, 3), padding='SAME', stride=(1, 1),
             initializer=tf.contrib.layers.xavier_initializer(), l2_strength=0.0, bias=0.0):
    """
    Convolution 2D Wrapper
    :param name: (string) The name scope provided by the upper tf.name_scope('name') as scope.
    :param x: (tf.tensor) The input to the layer (N, H, W, C).
    :param w: (tf.tensor) pretrained weights (if None, it means no pretrained weights)
    :param num_filters: (integer) No. of filters (This is the output depth)
    :param kernel_size: (integer tuple) The size of the convolving kernel.
    :param padding: (string) The amount of padding required.
    :param stride: (integer tuple) The stride required.
    :param initializer: (tf.contrib initializer) The initialization scheme, He et al. normal or Xavier normal are recommended.
    :param l2_strength:(weight decay) (float) L2 regularization parameter.
    :param bias: (float) Amount of bias. (if not float, it means pretrained bias)
    :return out: The output of the layer. (N, H', W', num_filters)
    """
    with tf.variable_scope(name):
        stride = [1, stride[0], stride[1], 1]
        kernel_shape = [kernel_size[0], kernel_size[1], x.shape[-1], num_filters]

        with tf.name_scope('layer_weights'):
            if w == None:
                w = variable_with_weight_decay(kernel_shape, initializer, l2_strength)
            variable_summaries(w)
        with tf.name_scope('layer_biases'):
            if isinstance(bias, float):
                bias = tf.get_variable('biases', [num_filters], initializer=tf.constant_initializer(bias))
            variable_summaries(bias)
        with tf.name_scope('layer_conv2d'):
            conv = tf.nn.conv2d(x, w, stride, padding)
            out = tf.nn.bias_add(conv, bias)

    return out


def atrous_conv2d_p(name, x, w=None, num_filters=16, kernel_size=(3, 3), padding='SAME', dilation_rate=1,
                    initializer=tf.contrib.layers.xavier_initializer(), l2_strength=0.0, bias=0.0):
    """
    Atrous Convolution 2D Wrapper
    :param name: (string) The name scope provided by the upper tf.name_scope('name') as scope.
    :param x: (tf.tensor) The input to the layer (N, H, W, C).
    :param w: (tf.tensor) pretrained weights
    :param num_filters: (integer) No. of filters (This is the output depth)
    :param kernel_size: (integer tuple) The size of the convolving kernel.
    :param padding: (string) The amount of padding required.
    :param dilation_rate: (integer) The amount of dilation required. If equals 1, it means normal convolution.
    :param initializer: (tf.contrib initializer) The initialization scheme, He et al. normal or Xavier normal are recommended.
    :param l2_strength:(weight decay) (float) L2 regularization parameter.
    :param bias: (float) Amount of bias. (if not float, it means pretrained bias)
    :return out: The output of the layer. (N, H', W', num_filters)
    """
    with tf.variable_scope(name):
        kernel_shape = [kernel_size[0], kernel_size[1], x.shape[-1], num_filters]

        with tf.name_scope('layer_weights'):
            if w == None:
                w = variable_with_weight_decay(kernel_shape, initializer, l2_strength)
            variable_summaries(w)
        with tf.name_scope('layer_biases'):
            if isinstance(bias, float):
                bias = tf.get_variable('biases', [num_filters], initializer=tf.constant_initializer(bias))
            variable_summaries(bias)
        with tf.name_scope('layer_atrous_conv2d'):
            conv = tf.nn.atrous_conv2d(x, w, dilation_rate, padding)
            out = tf.nn.bias_add(conv, bias)

    return out


def conv2d_transpose_p(name, x, w=None, output_shape=None, kernel_size=(3, 3), padding='SAME', stride=(1, 1),
                       l2_strength=0.0,
                       bias=0.0):
    """
    Convolution Transpose 2D Wrapper
    :param name: (string) The name scope provided by the upper tf.name_scope('name') as scope.
    :param x: (tf.tensor) The input to the layer (N, H, W, C).
    :param output_shape: (Array) [N, H', W', C'] The shape of the corresponding output.
    :param kernel_size: (integer tuple) The size of the convolving kernel.
    :param padding: (string) The amount of padding required.
    :param stride: (integer tuple) The stride required.
    :param l2_strength:(weight decay) (float) L2 regularization parameter.
    :param bias: (float) Amount of bias. (if not float, it means pretrained bias)
    :return out: The output of the layer. (output_shape[0], output_shape[1], output_shape[2], output_shape[3])
    """
    with tf.variable_scope(name):
        stride = [1, stride[0], stride[1], 1]
        kernel_shape = [kernel_size[0], kernel_size[1], output_shape[-1], x.shape[-1]]
        if w == None:
            w = get_deconv_filter(kernel_shape, l2_strength)
        variable_summaries(w)
        deconv = tf.nn.conv2d_transpose(x, w, tf.stack(output_shape), strides=stride, padding=padding)
        if isinstance(bias, float):
            bias = tf.get_variable('layer_biases', [output_shape[-1]], initializer=tf.constant_initializer(bias))
        variable_summaries(bias)
        out = tf.nn.bias_add(deconv, bias)

    return out


def conv2d(name, x, w=None, num_filters=16, kernel_size=(3, 3), padding='SAME', stride=(1, 1),
           initializer=tf.contrib.layers.xavier_initializer(), l2_strength=0.0, bias=0.0,
           activation=None, batchnorm_enabled=False, max_pool_enabled=False, dropout_keep_prob=-1,
           is_training=True):
    """
    This block is responsible for a convolution 2D layer followed by optional (non-linearity, dropout, max-pooling).
    Note that: "is_training" should be passed by a correct value based on being in either training or testing.
    :param name: (string) The name scope provided by the upper tf.name_scope('name') as scope.
    :param x: (tf.tensor) The input to the layer (N, H, W, C).
    :param num_filters: (integer) No. of filters (This is the output depth)
    :param kernel_size: (integer tuple) The size of the convolving kernel.
    :param padding: (string) The amount of padding required.
    :param stride: (integer tuple) The stride required.
    :param initializer: (tf.contrib initializer) The initialization scheme, He et al. normal or Xavier normal are recommended.
    :param l2_strength:(weight decay) (float) L2 regularization parameter.
    :param bias: (float) Amount of bias.
    :param activation: (tf.graph operator) The activation function applied after the convolution operation. If None, linear is applied.
    :param batchnorm_enabled: (boolean) for enabling batch normalization.
    :param max_pool_enabled:  (boolean) for enabling max-pooling 2x2 to decrease width and height by a factor of 2.
    :param dropout_keep_prob: (float) for the probability of keeping neurons. If equals -1, it means no dropout
    :param is_training: (boolean) to diff. between training and testing (important for batch normalization and dropout)
    :return: The output tensor of the layer (N, H', W', C').
    """
    with tf.variable_scope(name) as scope:
        conv_o_b = conv2d_p(scope, x=x, w=w, num_filters=num_filters, kernel_size=kernel_size, stride=stride,
                            padding=padding,
                            initializer=initializer, l2_strength=l2_strength, bias=bias)

        if batchnorm_enabled:
            conv_o_bn = tf.layers.batch_normalization(conv_o_b, training=is_training)
            if not activation:
                conv_a = conv_o_bn
            else:
                conv_a = activation(conv_o_bn)
        else:
            if not activation:
                conv_a = conv_o_b
            else:
                conv_a = activation(conv_o_b)

        if dropout_keep_prob != -1:
            conv_o_dr = tf.nn.dropout(conv_a, dropout_keep_prob)
        else:
            conv_o_dr = conv_a

        conv_o = conv_o_dr
        if max_pool_enabled:
            conv_o = max_pool_2d(scope, conv_o_dr)

    return conv_o


def atrous_conv2d(name, x, w=None, num_filters=16, kernel_size=(3, 3), padding='SAME', dilation_rate=1,
                  initializer=tf.contrib.layers.xavier_initializer(), l2_strength=0.0, bias=0.0,
                  activation=None, batchnorm_enabled=False, max_pool_enabled=False, dropout_keep_prob=-1,
                  is_training=True):
    """
    This block is responsible for a Dilated convolution 2D layer followed by optional (non-linearity, dropout, max-pooling).
    Note that: "is_training" should be passed by a correct value based on being in either training or testing.
    :param name: (string) The name scope provided by the upper tf.name_scope('name') as scope.
    :param x: (tf.tensor) The input to the layer (N, H, W, C).
    :param num_filters: (integer) No. of filters (This is the output depth)
    :param kernel_size: (integer tuple) The size of the convolving kernel.
    :param padding: (string) The amount of padding required.
    :param dilation_rate: (integer) The amount of dilation required. If equals 1, it means normal convolution.
    :param initializer: (tf.contrib initializer) The initialization scheme, He et al. normal or Xavier normal are recommended.
    :param l2_strength:(weight decay) (float) L2 regularization parameter.
    :param bias: (float) Amount of bias.
    :param activation: (tf.graph operator) The activation function applied after the convolution operation. If None, linear is applied.
    :param batchnorm_enabled: (boolean) for enabling batch normalization.
    :param max_pool_enabled:  (boolean) for enabling max-pooling 2x2 to decrease width and height by a factor of 2.
    :param dropout_keep_prob: (float) for the probability of keeping neurons. If equals -1, it means no dropout
    :param is_training: (boolean) to diff. between training and testing (important for batch normalization and dropout)
    :return: The output tensor of the layer (N, H', W', C').
    """
    with tf.variable_scope(name) as scope:
        conv_o_b = atrous_conv2d_p(scope, x=x, w=w, num_filters=num_filters, kernel_size=kernel_size,
                                   padding=padding, dilation_rate=dilation_rate,
                                   initializer=initializer, l2_strength=l2_strength, bias=bias)

        if batchnorm_enabled:
            conv_o_bn = tf.layers.batch_normalization(conv_o_b, training=is_training)
            if not activation:
                conv_a = conv_o_bn
            else:
                conv_a = activation(conv_o_bn)
        else:
            if not activation:
                conv_a = conv_o_b
            else:
                conv_a = activation(conv_o_b)

        if dropout_keep_prob != -1:
            conv_o_dr = tf.nn.dropout(conv_a, dropout_keep_prob)
        else:
            conv_o_dr = conv_a

        conv_o = conv_o_dr
        if max_pool_enabled:
            conv_o = max_pool_2d(scope, conv_o_dr)

    return conv_o


def conv2d_transpose(name, x, w=None, output_shape=None, kernel_size=(3, 3), padding='SAME', stride=(1, 1),
                     l2_strength=0.0,
                     bias=0.0, activation=None, batchnorm_enabled=False, dropout_keep_prob=-1,
                     is_training=True):
    """
    This block is responsible for a convolution transpose 2D followed by optional (non-linearity, dropout, max-pooling).
    Note that: "is_training" should be passed by a correct value based on being in either training or testing.
    :param name: (string) The name scope provided by the upper tf.name_scope('name') as scope.
    :param x: (tf.tensor) The input to the layer (N, H, W, C).
    :param output_shape: (Array) [N, H', W', C'] The shape of the corresponding output.
    :param kernel_size: (integer tuple) The size of the convolving kernel.
    :param padding: (string) The amount of padding required.
    :param stride: (integer tuple) The stride required.
    :param l2_strength:(weight decay) (float) L2 regularization parameter.
    :param bias: (float) Amount of bias.
    :param activation: (tf.graph operator) The activation function applied after the convolution operation. If None, linear is applied.
    :param batchnorm_enabled: (boolean) for enabling batch normalization.
    :param max_pool_enabled:  (boolean) for enabling max-pooling 2x2 to decrease width and height by a factor of 2.
    :param dropout_keep_prob: (float) for the probability of keeping neurons. If equals -1, it means no dropout
    :param is_training: (boolean) to diff. between training and testing (important for batch normalization and dropout)
    :return out: The output of the layer. (output_shape[0], output_shape[1], output_shape[2], output_shape[3])
    """
    with tf.variable_scope(name) as scope:
        conv_o_b = conv2d_transpose_p(name=scope, x=x, w=w, output_shape=output_shape, kernel_size=kernel_size,
                                      padding=padding, stride=stride,
                                      l2_strength=l2_strength,
                                      bias=bias)

        if batchnorm_enabled:
            conv_o_bn = tf.layers.batch_normalization(conv_o_b, training=is_training)
            if not activation:
                conv_a = conv_o_bn
            else:
                conv_a = activation(conv_o_bn)
        else:
            if not activation:
                conv_a = conv_o_b
            else:
                conv_a = activation(conv_o_b)

        if dropout_keep_prob != -1:
            conv_o_dr = tf.nn.dropout(conv_a, dropout_keep_prob)
        else:
            conv_o_dr = conv_a

        conv_o = conv_o_dr

        return conv_o


#############################################################################################################
# Dense Layer methods

def dense_p(name, x, w=None, output_dim=128, initializer=tf.contrib.layers.xavier_initializer(), l2_strength=0.0,
            bias=0.0):
    """
    Fully connected layer
    :param name: (string) The name scope provided by the upper tf.name_scope('name') as scope.
    :param x: (tf.tensor) The input to the layer (N, D).
    :param output_dim: (integer) It specifies H, the output second dimension of the fully connected layer [ie:(N, H)]
    :param initializer: (tf.contrib initializer) The initialization scheme, He et al. normal or Xavier normal are recommended.
    :param l2_strength:(weight decay) (float) L2 regularization parameter.
    :param bias: (float) Amount of bias. (if not float, it means pretrained bias)
    :return out: The output of the layer. (N, H)
    """
    n_in = x.get_shape()[-1].value
    with tf.variable_scope(name):
        if w == None:
            w = variable_with_weight_decay([n_in, output_dim], initializer, l2_strength)
        variable_summaries(w)
        if isinstance(bias, float):
            bias = tf.get_variable("layer_biases", [output_dim], tf.float32, tf.constant_initializer(bias))
        variable_summaries(bias)
        output = tf.nn.bias_add(tf.matmul(x, w), bias)
        return output


def dense(name, x, w=None, output_dim=128, initializer=tf.contrib.layers.xavier_initializer(), l2_strength=0.0,
          bias=0.0,
          activation=None, batchnorm_enabled=False, dropout_keep_prob=-1,
          is_training=True
          ):
    """
    This block is responsible for a fully connected followed by optional (non-linearity, dropout, max-pooling).
    Note that: "is_training" should be passed by a correct value based on being in either training or testing.
    :param name: (string) The name scope provided by the upper tf.name_scope('name') as scope.
    :param x: (tf.tensor) The input to the layer (N, D).
    :param output_dim: (integer) It specifies H, the output second dimension of the fully connected layer [ie:(N, H)]
    :param initializer: (tf.contrib initializer) The initialization scheme, He et al. normal or Xavier normal are recommended.
    :param l2_strength:(weight decay) (float) L2 regularization parameter.
    :param bias: (float) Amount of bias.
    :param activation: (tf.graph operator) The activation function applied after the convolution operation. If None, linear is applied.
    :param batchnorm_enabled: (boolean) for enabling batch normalization.
    :param dropout_keep_prob: (float) for the probability of keeping neurons. If equals -1, it means no dropout
    :param is_training: (boolean) to diff. between training and testing (important for batch normalization and dropout)
    :return out: The output of the layer. (N, H)
    """
    with tf.variable_scope(name) as scope:
        dense_o_b = dense_p(name=scope, x=x, w=w, output_dim=output_dim, initializer=initializer,
                            l2_strength=l2_strength,
                            bias=bias)

        if batchnorm_enabled:
            dense_o_bn = tf.layers.batch_normalization(dense_o_b, training=is_training)
            if not activation:
                dense_a = dense_o_bn
            else:
                dense_a = activation(dense_o_bn)
        else:
            if not activation:
                dense_a = dense_o_b
            else:
                dense_a = activation(dense_o_b)

        if dropout_keep_prob != -1:
            dense_o_dr = tf.nn.dropout(dense_a, dropout_keep_prob)
        else:
            dense_o_dr = dense_a

        dense_o = dense_o_dr
    return dense_o


def flatten(x):
    """
    Flatten a (N,H,W,C) input into (N,D) output. Used for fully connected layers after conolution layers
    :param x: (tf.tensor) representing input
    :return: flattened output
    """
    all_dims_exc_first = np.prod([v.value for v in x.get_shape()[1:]])
    o = tf.reshape(x, [-1, all_dims_exc_first])
    return o


#############################################################################################################
# Pooling Layers methods

def max_pool_2d(x, size=(2, 2)):
    """
    Max pooling 2D Wrapper
    :param x: (tf.tensor) The input to the layer (N,H,W,C).
    :param size: (tuple) This specifies the size of the filter as well as the stride.
    :return: The output is the same input but halfed in both width and height (N,H/2,W/2,C).
    """
    size_x, size_y = size
    return tf.nn.max_pool(x, ksize=[1, size_x, size_y, 1], strides=[1, size_x, size_y, 1], padding='VALID',
                          name='pooling')


def upsample_2d(x, size=(2, 2)):
    """
    Bilinear Upsampling 2D Wrapper
    :param x: (tf.tensor) The input to the layer (N,H,W,C).
    :param size: (tuple) This specifies the size of the filter as well as the stride.
    :return: The output is the same input but doubled in both width and height (N,2H,2W,C).
    """
    h, w, _ = x.get_shape().as_list()[1:]
    size_x, size_y = size
    output_h = h * size_x
    output_w = w * size_y
    return tf.image.resize_bilinear(x, (output_h, output_w), align_corners=None, name='upsampling')


#############################################################################################################
# Utils for Layers methods

def variable_with_weight_decay(kernel_shape, initializer, wd):
    """
    Create a variable with L2 Regularization (Weight Decay)
    :param kernel_shape: the size of the convolving weight kernel.
    :param initializer: The initialization scheme, He et al. normal or Xavier normal are recommended.
    :param wd:(weight decay) L2 regularization parameter.
    :return: The weights of the kernel initialized. The L2 loss is added to the loss collection.
    """
    w = tf.get_variable('weights', kernel_shape, tf.float32, initializer=initializer)

    collection_name = tf.GraphKeys.REGULARIZATION_LOSSES
    if wd and (not tf.get_variable_scope().reuse):
        weight_decay = tf.multiply(tf.nn.l2_loss(w), wd, name='w_loss')
        tf.add_to_collection(collection_name, weight_decay)
    variable_summaries(w)
    return w


# Summaries for variables
def variable_summaries(var):
    """
    Attach a lot of summaries to a Tensor (for TensorBoard visualization).
    :param var: variable to be summarized
    :return: None
    """
    with tf.name_scope('summaries'):
        mean = tf.reduce_mean(var)
        tf.summary.scalar('mean', mean)
        with tf.name_scope('stddev'):
            stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))
        tf.summary.scalar('stddev', stddev)
        tf.summary.scalar('max', tf.reduce_max(var))
        tf.summary.scalar('min', tf.reduce_min(var))
        tf.summary.histogram('histogram', var)


def get_deconv_filter(f_shape, l2_strength):
    """
    The initializer for the bilinear convolution transpose filters
    :param f_shape: The shape of the filter used in convolution transpose.
    :param l2_strength: L2 regularization parameter.
    :return weights: The initialized weights.
    """
    width = f_shape[0]
    height = f_shape[0]
    f = math.ceil(width / 2.0)
    c = (2 * f - 1 - f % 2) / (2.0 * f)
    bilinear = np.zeros([f_shape[0], f_shape[1]])
    for x in range(width):
        for y in range(height):
            value = (1 - abs(x / f - c)) * (1 - abs(y / f - c))
            bilinear[x, y] = value
    weights = np.zeros(f_shape)
    for i in range(f_shape[2]):
        weights[:, :, i, i] = bilinear

    init = tf.constant_initializer(value=weights, dtype=tf.float32)
    return variable_with_weight_decay(weights.shape, init, l2_strength)


def noise_and_argmax(logits):
    # Add noise then take the argmax
    noise = tf.random_uniform(tf.shape(logits))
    return tf.argmax(logits - tf.log(-tf.log(noise)), 1)


#def openai_entropy(logits):
#    # Entropy proposed by OpenAI in their A2C baseline
#    a0 = logits - tf.reduce_max(logits, 1, keep_dims=True)
#    ea0 = tf.exp(a0)
#    z0 = tf.reduce_sum(ea0, 1, keep_dims=True)
#    p0 = ea0 / z0
#    return tf.reduce_sum(p0 * (tf.log(z0) - a0), 1)


def softmax_entropy(p0):
    # Normal information theory entropy by Shannon
    return - tf.reduce_sum(p0 * tf.log(p0 + 1e-6), axis=1)


def mse(predicted, ground_truth):
    # Mean-squared error
    return tf.square(predicted - ground_truth) / 2.


def orthogonal_initializer(scale=1.0):
    def _ortho_init(shape, dtype, partition_info=None):
        # Orthogonal Initializer that uses SVD. The unused variables are just for passing in tensorflow
        shape = tuple(shape)
        if len(shape) == 2:
            flat_shape = shape
        elif len(shape) == 4:  # assumes NHWC
            flat_shape = (np.prod(shape[:-1]), shape[-1])
        else:
            raise NotImplementedError
        a = np.random.normal(0.0, 1.0, flat_shape)
        u, _, v = np.linalg.svd(a, full_matrices=False)
        q = u if u.shape == flat_shape else v  # pick the one with the correct shape
        q = q.reshape(shape)
        return (scale * q[:shape[0], :shape[1]]).astype(np.float32)

    return _ortho_init
###########################################################################
import numpy as np
import tensorflow as tf
import sys, gym, tqdm, argparse, json, easydict, os, time, random, pprint
import matplotlib.pyplot as plt
from itertools import cycle
maxep = 1600
###########################################################################
def s_reshaper(s):
    return np.reshape(s, (1, 4))
def openai_entropy(logits):# Entropy proposed by OpenAI in their A2C baseline
    a0 = logits - tf.reduce_max(logits, 1, keep_dims=True)
    ea0 = tf.exp(a0)
    z0 = tf.reduce_sum(ea0, 1, keep_dims=True)
    p0 = ea0 / z0
    return tf.reduce_sum(p0 * (tf.log(z0) - a0), 1)
class Model:
    def __init__(self,env,unitslist):
        #self.input_actor  = tf.placeholder(tf.float32, [1, 4], name='input_actor')
        #self.target_actor = tf.placeholder(tf.float32, [1, env.action_space.n], name='target_actor')
        #self.input_critic = tf.placeholder(tf.float32, [1, 4], name='input_critic')
        #self.target_critic= tf.placeholder(tf.float32, [1, 1], name='target_critic')
        if 1:
            self.input_actor  = tf.placeholder(tf.float32, [1, 4], name='input_actor')
            self.target_actor = tf.placeholder(tf.float32, [1, env.action_space.n], name='target_actor')
            initializer = None#tf.random_uniform() #tf.constant_initializer(1.0)
            self.ah_values = []
            ah_value = self.input_actor
            self.ah_values.append(ah_value)
            for i,units in enumerate(unitslist[1:len(unitslist)-1]):
                if i==0:
                    ah_value = tf.layers.dense(inputs=ah_value, units=units, activation=None, kernel_initializer=initializer, bias_initializer=initializer)
                    ah_value = tf.layers.batch_normalization(inputs=ah_value,scale=False,training=True)
                    ah_value = tf.nn.relu(ah_value)
                #h_valueo= tf.identity(h_value)
                ah_value = tf.layers.batch_normalization(inputs=ah_value,scale=False,training=True)#tf.contrib.layers.batch_norm(h_value)
                ah_value = tf.nn.relu(ah_value)
                ah_value = tf.layers.dense(inputs=ah_value, units=units, activation=None, kernel_initializer=initializer, bias_initializer=initializer)
                ah_value = tf.layers.batch_normalization(inputs=ah_value,scale=False,training=True)
                ah_value = tf.nn.relu(ah_value)
                ah_value = tf.layers.dense(inputs=ah_value, units=units, activation=None, kernel_initializer=initializer, bias_initializer=initializer)
                #h_value = h_value + h_valueo
                self.ah_values.append(ah_value)
            self.ay_value = tf.layers.dense(inputs=ah_value, units=2, activation=None, kernel_initializer=initializer, bias_initializer=initializer)#tf.nn.softmax
            self.ah_values.append(self.ay_value)
            self.policy           = tf.nn.softmax(self.ay_value)
            self.loss_actor       = tf.reduce_sum(tf.nn.softmax_cross_entropy_with_logits(logits=self.ay_value, labels=self.target_actor))

            self.input_critic = tf.placeholder(tf.float32, [1, 4], name='input_critic')
            self.target_critic= tf.placeholder(tf.float32, [1, 1], name='target_critic')
            initializer = None#tf.random_uniform() #tf.constant_initializer(1.0)
            self.ch_values = []
            ch_value = self.input_critic
            self.ch_values.append(ch_value)
            for i,units in enumerate(unitslist[1:len(unitslist)-1]):
                if i==0:
                    ch_value = tf.layers.dense(inputs=ch_value, units=units, activation=None, kernel_initializer=initializer, bias_initializer=initializer)
                    ch_value = tf.layers.batch_normalization(inputs=ch_value,scale=False,training=True)
                    ch_value = tf.nn.relu(ch_value)
                #h_valueo= tf.identity(h_value)
                ch_value = tf.layers.batch_normalization(inputs=ch_value,scale=False,training=True)#tf.contrib.layers.batch_norm(h_value)
                ch_value = tf.nn.relu(ch_value)
                ch_value = tf.layers.dense(inputs=ch_value, units=units, activation=None, kernel_initializer=initializer, bias_initializer=initializer)
                ch_value = tf.layers.batch_normalization(inputs=ch_value,scale=False,training=True)
                ch_value = tf.nn.relu(ch_value)
                ch_value = tf.layers.dense(inputs=ch_value, units=units, activation=None, kernel_initializer=initializer, bias_initializer=initializer)
                #h_value = h_value + h_valueo
                self.ch_values.append(ch_value)
            self.cy_value = tf.layers.dense(inputs=ch_value, units=1, activation=None, kernel_initializer=initializer, bias_initializer=initializer)#tf.nn.softmax
            self.ch_values.append(self.cy_value)
            self.value_function   = self.cy_value
            self.loss_critic      = tf.reduce_sum(tf.square(self.target_critic - self.value_function))

            if 0:
                self.input_actor  = tf.placeholder(tf.float32, [1, 4], name='input_actor')
                self.target_actor = tf.placeholder(tf.float32, [1, env.action_space.n], name='target_actor')
                l1 = tf.layers.dense(inputs=self.input_actor, units=unitslist[0], activation=tf.nn.relu, 
                    kernel_initializer=tf.random_normal_initializer(0., .1), bias_initializer=tf.constant_initializer(0.1))
                l2 = tf.layers.dense(inputs=l1, units=env.action_space.n, activation=None, 
                    kernel_initializer=tf.random_normal_initializer(0., .1), bias_initializer=tf.constant_initializer(0.1))
                self.policy           = tf.nn.softmax(l2)
                self.loss_actor       = tf.reduce_sum(tf.nn.softmax_cross_entropy_with_logits(logits=l2, labels=self.target_actor))

                self.input_critic = tf.placeholder(tf.float32, [1, 4], name='input_critic')
                self.target_critic= tf.placeholder(tf.float32, [1, 1], name='target_critic')
                cl1 = tf.layers.dense(inputs=self.input_critic, units=unitslist[0], activation=tf.nn.relu, 
                    kernel_initializer=tf.random_normal_initializer(0., .1), bias_initializer=tf.constant_initializer(0.1))
                cl2 = tf.layers.dense(inputs=cl1, units=1, activation=None, 
                    kernel_initializer=tf.random_normal_initializer(0., .1), bias_initializer=tf.constant_initializer(0.1))
                self.value_function   = cl2
                self.loss_critic      = tf.reduce_sum(tf.square(self.target_critic - self.value_function))
        else:
            self.input_actor  = tf.placeholder(tf.float32, [1, 4], name='input_actor')
            self.target_actor = tf.placeholder(tf.float32, [1, env.action_space.n], name='target_actor')
            self.W1_actor = tf.Variable(tf.truncated_normal([4, unitslist[0]]))
            self.HL_actor = tf.nn.relu(tf.matmul(self.input_actor, self.W1_actor))
            self.W2_actor = tf.Variable(tf.truncated_normal([unitslist[0], 2]))
            self.policy = tf.nn.softmax(tf.matmul(self.HL_actor, self.W2_actor))
            self.loss_actor = tf.reduce_sum( tf.nn.softmax_cross_entropy_with_logits(logits=tf.matmul(self.HL_actor, self.W2_actor), labels=self.target_actor))

            self.input_critic = tf.placeholder(tf.float32, [1, 4], name='input_critic')
            self.target_critic= tf.placeholder(tf.float32, [1, 1], name='target_critic')
            self.W1_critic = tf.Variable(tf.truncated_normal([4, unitslist[0]]))
            self.HL_critic = tf.nn.relu(tf.matmul(self.input_critic, self.W1_critic))
            self.W2_critic = tf.Variable(tf.truncated_normal([unitslist[0], 1]))
            self.value_function = tf.matmul(self.HL_critic, self.W2_critic)
            self.loss_critic = tf.reduce_sum(tf.square(self.target_critic - self.value_function))
        if 0:
            #negative_log_prob_action  = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=l2, labels=self.actions)
            #self.policy_gradient_loss = tf.reduce_mean(self.advantages * negative_log_prob_action)
            self.policy_gradient_loss = self.loss_actor
            #self.value_function_loss  = tf.reduce_mean(mse(tf.squeeze(self.value_function), self.rewards))
            self.value_function_loss  = self.loss_critic
            #self.entropy              = tf.reduce_mean(openai_entropy(l2))
            self.entropy_coeff, self.valuefc_coeff, self.max_grad_norm = 0.01, 1.0, 0.5#0.01, 0.5, 0.5 ############
            self.loss = self.policy_gradient_loss*0.1 + self.value_function_loss*self.valuefc_coeff# - self.entropy*self.entropy_coeff
###########################################################################
"""class CNNPolicy:
    def __init__(self, args, sess, X_input, num_actions, reuse=False, is_training=False):
        self.sess = sess
        self.P_input = X_input
        self.cast = tf.transpose(self.P_input[:,:,:,:,0],[0,2,3,1])
        self.cast = tf.cast(self.cast, tf.float32) / 255.0
        self.convs, self.convouts = list(args.convs), []
        self.denss, self.densouts = list(args.denss), []
        with tf.variable_scope("policy", reuse=reuse):
            self.conv = self.cast
            for i,conv in enumerate(self.convs):
                convout = conv2d('conv'+str(i), self.conv, num_filters=conv[0], kernel_size=(conv[1], conv[2]), padding='VALID', stride=(conv[3], conv[4]),
                        initializer=orthogonal_initializer(np.sqrt(2)), activation=tf.nn.relu, is_training=is_training)
                self.conv = convout
                self.convouts.append(convout)
            self.conv_flattened = flatten(self.conv)
            self.dens = self.conv_flattened
            for i,dens in enumerate(self.denss):
                densout = dense('dens'+str(i), self.dens, output_dim=dens,
                        initializer=orthogonal_initializer(np.sqrt(2)), activation=tf.nn.relu, is_training=is_training)
                self.dens = densout
                self.densouts.append(densout)
            self.policy_logits  = dense('policy_logits',  self.dens, output_dim=num_actions, initializer=orthogonal_initializer(np.sqrt(1.0)), is_training=is_training)
            self.value_function = dense('value_function', self.dens, output_dim=1          , initializer=orthogonal_initializer(np.sqrt(1.0)), is_training=is_training)
            with tf.name_scope('action'):
                self.action_s = noise_and_argmax(self.policy_logits)
            with tf.name_scope('value'):
                self.value_s = self.value_function[:, 0]
    def step(self, observation, *_args, **_kwargs):
        #self.debug(observation)
        action, value = self.sess.run([self.action_s, self.value_s], {self.P_input: observation})
        return action, value, []  # dummy state
    def value(self, observation, *_args, **_kwargs):
        return self.sess.run(self.value_s, {self.P_input: observation})
    def debug(self, observation, *_args, **_kwargs):
        print(observation.shape)
        cast = self.sess.run([self.cast], {self.P_input: observation})
        print('cast',cast.shape)
        for i in range(len(self.convouts)):
            convi = self.sess.run([self.convouts[i]], {self.P_input: observation})
            print('conv'+str(i),convi.shape)
        conv_flattened = self.sess.run([self.conv_flattened], {self.P_input: observation})
        print('conv_flattened',conv_flattened.shape)
        for i in range(len(self.densouts)):
            densi = self.sess.run([self.densouts[i]], {self.P_input: observation})
            print('dens'+str(i),densi.shape)
        policy_logits, value_function, action_s, value_s = self.sess.run([self.policy_logits, self.value_function, self.action_s, self.value_s], {self.P_input: observation})
        print('policy_logits',policy_logits.shape)
        print('value_function',value_function.shape)
        print('action_s',action_s.shape)
        print('value_s',value_s.shape)
        exit()
class CNNModel:
    def __init__(self, sess, args, observation_space_params, num_actions):
        self.sess = sess
        self.input_shape   = [args.num_stack]+list(observation_space_params)
        self.num_actions   = num_actions
        self.policy        = CNNPolicy
        self.entropy_coeff = args.entropy_coef
        self.valuefc_coeff = args.value_function_coeff
        self.max_grad_norm = args.max_gradient_norm
        with tf.name_scope('train_input'):
            self.X_input        = tf.placeholder(tf.uint8, [None]+self.input_shape)
            self.actions        = tf.placeholder(tf.int32, [None])
            self.rewards        = tf.placeholder(tf.float32, [None])
            self.advantages     = tf.placeholder(tf.float32, [None])
            self.learning_rate  = tf.placeholder(tf.float32, [])
        self.step_policy  = self.policy(args, self.sess, self.X_input, self.num_actions, reuse=False, is_training=False)
        self.train_policy = self.policy(args, self.sess, self.X_input, self.num_actions, reuse=True , is_training=True)
        with tf.variable_scope('train'):
            negative_log_prob_action  = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=self.train_policy.policy_logits, labels=self.actions)
            self.policy_gradient_loss = tf.reduce_mean(self.advantages * negative_log_prob_action)
            self.value_function_loss  = tf.reduce_mean(mse(tf.squeeze(self.train_policy.value_function), self.rewards))
            self.entropy              = tf.reduce_mean(openai_entropy(self.train_policy.policy_logits))
            self.loss = self.policy_gradient_loss - self.entropy*self.entropy_coeff + self.value_function_loss*self.valuefc_coeff
            with tf.variable_scope("policy"):
                params = tf.trainable_variables()
                grads  = tf.gradients(self.loss, params)
                if self.max_grad_norm is not None: grads, grad_norm = tf.clip_by_global_norm(grads, self.max_grad_norm)
                grads  = list(zip(grads, params))
                self.optimize = tf.train.AdamOptimizer(learning_rate=self.learning_rate).apply_gradients(grads)"""
###########################################################################
class Trainer:
    def __init__(self,env,sess,model,expdir):
        self.GAMMA, self.penalize = 1.0, -100###
        self.env, self.sess, self.model, self.expdir = env, sess, model, expdir
        self.actlr = tf.placeholder(tf.float32, [])
        self.crtlr = tf.placeholder(tf.float32, [])
        if 0:
            #params = tf.trainable_variables()
            #grads  = tf.gradients(self.loss, params)
            #if self.max_grad_norm is not None: grads, grad_norm = tf.clip_by_global_norm(grads, self.max_grad_norm)
            #grads  = list(zip(grads, params))
            self.optimize = tf.train.AdamOptimizer(learning_rate=self.actlr).minimize(self.model.loss)#.apply_gradients(grads)
        self.train_actor_ops = tf.train.AdamOptimizer(self.actlr).minimize(self.model.loss_actor)
        self.train_critic_ops= tf.train.AdamOptimizer(self.crtlr).minimize(self.model.loss_critic)

        self.sess.run(tf.global_variables_initializer())
        # have to be linear to make sure the convergence of actor. But linear approximator seems hardly learns the correct Q.
    def update(self, s_old, a, r, s_new, done, actlr, crtlr):
        target_critic = np.zeros((1, 1))
        target_actor  = np.zeros((1, self.env.action_space.n))
        V_s_old = self.sess.run(self.model.value_function, feed_dict={self.model.input_critic: s_old})
        V_s_new = self.sess.run(self.model.value_function, feed_dict={self.model.input_critic: s_new})
        if done: V_s_new = 0.0 # The value function of s_new must be zero because the state leads to game end
        #if do not do this, experiments show that at last there will be a good probability that every episode end at only 9-10 steps
        target_critic[0][0]= r + self.GAMMA * V_s_new
        target_actor[0][a] = r + self.GAMMA * V_s_new - V_s_old
        self.sess.run([self.train_critic_ops, self.train_actor_ops], 
            feed_dict={ self.model.input_critic: s_old, self.model.target_critic: target_critic,
                        self.model.input_actor: s_old, self.model.target_actor: target_actor, self.actlr: actlr, self.crtlr: crtlr})
    def fit(self,LR_A,LR_C,MAX_STEPS):
        flog= open("a2ccurve",'a')
        print('',file=flog)
        solved = False
        episode_rewards=[0.0]
        obs = s_reshaper(self.env.reset())
        for t in tqdm.tqdm(range(MAX_STEPS)):
            action = np.random.choice([0, 1], 1, p=self.sess.run(self.model.policy, feed_dict={self.model.input_actor: obs}).ravel())[0]
            new_obs, rew, done, info = self.env.step(action)
            new_obs = s_reshaper(new_obs)
            episode_rewards[-1]+=rew
            if done and episode_rewards[-1]<500: rew = self.penalize###

            actlr, crtlr = LR_A, LR_C#*(1-t/MAX_STEPS), LR_C*(1-t/MAX_STEPS)
            if not solved: self.update(obs, action, rew, new_obs, done and episode_rewards[-1]<500, actlr, crtlr)

            obs = new_obs
            if done:
                print(int(episode_rewards[-1]),end='|',file=flog,flush=True)
                if np.mean(episode_rewards[-min(5, len(episode_rewards)):]) > 490: solved = True
                else: solved = False
                if len(episode_rewards) >= maxep: break
                episode_rewards.append(0.0)
                obs = s_reshaper(self.env.reset())
        flog.close()
        return episode_rewards
###########################################################################
def setsession(randseed):
    tfconfig = tf.ConfigProto(device_count={'GPU': 0})
    sess = tf.Session(config=tfconfig)
    random.seed(randseed)
    np.random.seed(randseed)
    tf.set_random_seed(randseed)
    return sess
###########################################################################
def main():
    parser = argparse.ArgumentParser(description="a2ctest")
    parser.add_argument('--config', default=None, type=str, help='Configuration file')
    parser.add_argument('--seed', default=666, type=str, help='Env seed')
    parser.add_argument('--finalseed', default=666, type=str, help='Env finalseed')
    parser.add_argument('--to_train', default=False, type=str, help='To train')
    args = parser.parse_args()
    with open(args.config, 'r') as config_file:
        config_args_dict = json.load(config_file)
    config_args = easydict.EasyDict(config_args_dict)
    with open('./myenv/envinfo.json', 'w') as fenvinfo:
        print(json.dumps(config_args),file=fenvinfo)
    config_args.env_seed       = int(args.seed)
    if args.to_train == 'True': config_args.to_train = True
    else:                       config_args.to_train = False
    config_args.experiment_dir = "./"#args.config.replace('.', '_') + str(config_args.env_seed) + "/"
    config_args.checkpoint_dir = config_args.experiment_dir + 'checkpoints/'
    config_args.summary_dir    = config_args.experiment_dir + 'summaries/'
    config_args.output_dir     = config_args.experiment_dir + 'output/'
    config_args.test_dir       = config_args.experiment_dir + 'test/'
    dirs = [config_args.checkpoint_dir, config_args.summary_dir, config_args.output_dir, config_args.test_dir]
    #for dir_ in dirs:
    #    if not os.path.exists(dir_): os.makedirs(dir_)
    flog = open(config_args.experiment_dir+'log','a')
    starttime = time.time()

    sess = setsession(config_args.env_seed)
    env  = gym.make('CartPole-v1')#shipS-v0
    env.seed(config_args.env_seed)

    unitslist, MAX_STEPS, LR_A, LR_C = [4,128,32,8,2], 300000, 0.1, 0.5#0.01
    # we need a good teacher, so the teacher should learn faster than the actor
    #ob_shape, act_n = env.observation_space.shape[0], env.action_space.n
    model   = Model(env,unitslist)
    trainer = Trainer(env,sess,model,config_args.experiment_dir)
    print(args.seed,':',args.finalseed)
    episode_rewards = trainer.fit(LR_A, LR_C, MAX_STEPS)

    pprint.pprint(config_args,flog)
    endtime = time.time()
    print(time.ctime(starttime),file=flog)
    print(time.ctime(endtime),file=flog)
    print((endtime-starttime)/60,'minutes',file=flog)
    print((endtime-starttime)/3600,'hours',file=flog)
    flog.close()

    doneep = maxep
    avgnum = 50
    episode_rewards.reverse()
    record = episode_rewards
    recordmean = [np.mean(record[i:i+avgnum]) for i in range(len(record)-avgnum+1)]
    for i,record in enumerate(recordmean):
        if record < 490:
            doneep = i
            break
    fdoneeps = open('doneeps','a')
    print(doneep,end=',',file=fdoneeps)
    fdoneeps.close()
    if args.seed==args.finalseed or int(args.seed)%10==0:
        fdoneeps = open('doneeps','r')
        doneeps  = [int(doneep) for doneep in fdoneeps.read().splitlines()[0].split(",")[:-1]]
        plt.figure(111)
        print(doneeps)
        bins = np.linspace(0,maxep,50)
        plt.hist(doneeps, bins=bins, normed=False,facecolor='r',edgecolor='r',hold=0,alpha=0.2)
        figname = 'fdoneeps'+".png"
        plt.savefig(figname, figsize=(16, 9), dpi=300, facecolor="azure", bbox_inches='tight', pad_inches=0)

        COLORS = cycle(['black', 'red', 'orange', 'green', 'cyan', 'blue', 'purple'])
        flog=open("a2ccurve","r")
        lines=flog.read().splitlines()
        flog.close()
        for i in range(0,len(lines)):
            color=next(COLORS)
            record = lines[i].split("|")
            record = record[:len(record)-1]
            record = [float(strs) for strs in record]
            avgnum = 50
            recordmean = [np.mean(record[i:i+avgnum]) for i in range(len(record)-avgnum+1)]
            plt.plot(record,label=i,color=color,alpha=0.1)
            plt.plot(recordmean,label=i,color=color,alpha=1.0)
        axes = plt.gca()
        plt.savefig('a2ccurve.png', figsize=(16, 9), dpi=300, facecolor="azure", bbox_inches='tight', pad_inches=0)

if __name__ == '__main__':
    main()
